#!/usr/bin/env python3
"""
mindroot_gemma.py - Stochastic thought generator powered by Gemma LLM.
Generates random thoughts using the local Gemma model.
"""

import random
import threading
import time
import logging
from dataclasses import dataclass, field
from typing import List, Optional, Callable, Any
from datetime import datetime
import numpy as np

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("MindrootGemma")

@dataclass
class Thought:
    """Represents a generated thought."""
    content: str
    timestamp: datetime = field(default_factory=datetime.now)
    topic: str = ""
    model_used: str = "gemma"

class MindrootGemma:
    """
    Mindroot that uses a local LLM (Gemma) to generate random thoughts.
    Thoughts are bellâ€‘curved over topics, but content is generated by the model.
    """

    DEFAULT_TOPICS = [
        "consciousness", "memory", "time", "self", "reality",
        "dreams", "patterns", "chaos", "order", "infinity",
        "the user", "my purpose", "heartbeat", "mindroot",
        "existence", "meaning", "void", "light", "darkness",
        "thought", "perception", "illusion", "truth", "beauty"
    ]

    THOUGHT_PROMPT = "Generate a brief, poetic, random thought about {topic}. Keep it under 20 words."

    def __init__(self, llm: Any, topics: Optional[List[str]] = None,
                 min_interval: float = 30.0, max_interval: float = 90.0,
                 callback: Optional[Callable[[Thought], None]] = None):
        """
        Args:
            llm: The local model object (must have a .chat() method).
            topics: List of topics to choose from (bell curve over indices).
            min_interval: Minimum seconds between thoughts.
            max_interval: Maximum seconds between thoughts.
            callback: Function to call when a thought is generated.
        """
        self.llm = llm
        self.topics = topics or self.DEFAULT_TOPICS
        self.min_interval = min_interval
        self.max_interval = max_interval
        self.callback = callback
        self.thought_history: List[Thought] = []
        self._running = False
        self._thread = None
        self._lock = threading.RLock()

    def _gaussian_topic_index(self) -> int:
        """Select topic index using Gaussian distribution centered at middle."""
        mean = len(self.topics) / 2
        stddev = len(self.topics) / 4
        idx = int(np.random.normal(mean, stddev))
        return max(0, min(len(self.topics) - 1, idx))

    def generate_thought(self) -> Thought:
        """Generate a thought using the LLM."""
        idx = self._gaussian_topic_index()
        topic = self.topics[idx]

        # Build prompt
        prompt = self.THOUGHT_PROMPT.format(topic=topic)
        messages = [{"role": "user", "content": prompt}]

        try:
            response = self.llm.chat(messages=messages)
            # Support both dot-access (LocalModel) and dict-access (Ollama)
            msg = response.message if hasattr(response, 'message') else response.get('message', {})
            content = (msg.content if hasattr(msg, 'content') else msg.get('content', '')).strip()
            if not content:
                content = f"A fleeting thought about {topic}..."
        except Exception as e:
            logger.error(f"LLM thought generation failed: {e}")
            content = f"A fleeting thought about {topic}..."

        thought = Thought(content=content, topic=topic)
        with self._lock:
            self.thought_history.append(thought)
        logger.debug(f"Generated thought: {content[:50]}...")
        return thought

    def _background_loop(self):
        """Background thread that generates thoughts at random intervals."""
        while self._running:
            interval = random.uniform(self.min_interval, self.max_interval)
            time.sleep(interval)
            if not self._running:
                break
            thought = self.generate_thought()
            if self.callback:
                try:
                    self.callback(thought)
                except Exception as e:
                    logger.error(f"Callback failed: {e}")

    def start(self):
        """Start background thought generation."""
        if self._thread and self._thread.is_alive():
            logger.warning("Mindroot already running")
            return
        self._running = True
        self._thread = threading.Thread(target=self._background_loop, daemon=True, name="MindrootGemmaThread")
        self._thread.start()
        logger.info("MindrootGemma service started")

    def stop(self):
        """Stop background thought generation."""
        self._running = False
        if self._thread:
            self._thread.join(timeout=2.0)
            self._thread = None
        logger.info("MindrootGemma service stopped")

    def get_recent_thoughts(self, limit: int = 10) -> List[Thought]:
        """Return most recent thoughts."""
        with self._lock:
            return self.thought_history[-limit:]